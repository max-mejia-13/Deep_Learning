# -*- coding: utf-8 -*-
"""Assignment_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VSIL8qQUorh4lUfyEJniRmulnPM1Uj5i

Maximo Mejia
Z23421055
https://colab.research.google.com/drive/1VSIL8qQUorh4lUfyEJniRmulnPM1Uj5i?usp=sharing
"""

import numpy as np
import matplotlib.pyplot as plt

# Neural Netword with single neuron
class NeuralNetwork():

  def __init__(self, learning_r):
    np.random.seed(1)
    self.weight_matrix = 2 * np.random.random((3,1)) - 1
    self.l_rate = learning_r
    weight_his = np.array([])
    cost = 0
    
# Sigmoid as an activation function
  def sigmoid(self, x):
    return 1/(1 + np.exp(-x))

  # forward propagation
  def forward_propagation(self, inputs):
    outs = np.dot(inputs, self.weight_matrix)
    return self.sigmoid(outs) 
  
  # training the neural network
  def train(self, train_inputs, train_labels, 
            num_train_iterations):
    N = train_inputs.shape[0]
    cost_func = np.array([])

    # Number of iterations we want to perform for this set of input.
    for iteration in range(num_train_iterations):
    
      outputs = self.forward_propagation(train_inputs)

      # Calculate error in the output
      error = train_labels - outputs
      adjustment = (self.l_rate/N)*np.sum(np.multiply(error, train_inputs),
                                          axis = 0)
      cost_func = np.append(cost_func, (1/2*N)*np.sum(np.power(error,2)))

      # Adjust the weigth matrix 
      self.weight_matrix[:, 0] += adjustment

    # plot the separating line based on weights
    print('Iteration #' + str(iteration))
    plot_fun_thr(train_inputs[:,1:3], train_labels[:,0],
                   self.weight_matrix[:,0], classes)
      
    plot_cost_func(cost_func, num_train_iterations)

  # predicting the classes of new data points
  def pred(self, inputs):
    preds = self.forward_propagation(inputs)
    return preds



 #Plot function   
def plot_fun_thr(features, labels, thre_parms, classes):

    plt.plot(features[labels[:] == classes[0], 0], 
             features[labels[:] == classes[0], 1], 'rs', 
             features[labels[:] == classes[1], 0], 
             features[labels[:] == classes[1], 1], 'g^')
    plt.axis([-5, 5, -5, 5])
    
    x1 = np.linspace(-5, 5, 50)
    x2 = - (thre_parms[1]*x1 + thre_parms[0])/thre_parms[2]

    plt.plot(x1, x2, '-r')
    plt.xlabel('x: feature 1')
    plt.ylabel('y: feature 2')
    plt.legend(['Class' + str(classes[0]), 'Class' + str(classes[1])])
    plt.grid()
    plt.show()

def plot_cost_func(J, iterations): 

  x = np.arange(iterations, dtype = int)
  y = J
  plt.plot(x, y)
  plt.axis([-1, x.shape[0] + 1, -1, np.max(y) + 1])
  plt.title('learning curve')
  plt.xlabel('x: iteration number')
  plt.ylabel('y: J(theta)')
  plt.show()

features = np.array([[1, 1], [1, 0], [0, 1], [0.5, -1],
                     [0.5, 3], [0.7, 2], [-1, 0], [-1, 1],
                     [2, 0], [0, 0]])
labels = np.array([1, 1, 0, 0, 1, 1, 0, 0, 1, 0])
classes = [0, 1]

bias = np.ones((features.shape[0], 1))
features = np.append(bias, features, axis = 1)
print(features.shape)

neural_network = NeuralNetwork(0.5)
print ('Random weights at the start of training')
print(neural_network.weight_matrix)
neural_network.train(features, np.expand_dims(labels, axis = 1), 30)

print ('New weights after training')
print (neural_network.weight_matrix)

features = np.array([[1, 1], [1, 0], [0, 1], [0.5, -1],
                     [0.5, 3], [0.7, 2], [-1, 0], [-1, 1],
                     [2, 0], [0, 0]])
labels = np.array([1, 1, 0, 0, 1, 1, 0, 0, 1, 0])
classes = [0, 1]

bias = np.ones((features.shape[0], 1))
features = np.append(bias, features, axis = 1)
print(features.shape)

neural_network = NeuralNetwork(0.1)
print ('Random weights at the start of training')
print(neural_network.weight_matrix)
neural_network.train(features, np.expand_dims(labels, axis = 1), 30)

print ('New weights after training')
print (neural_network.weight_matrix)

features = np.array([[1, 1], [1, 0], [0, 1], [0.5, -1],
                     [0.5, 3], [0.7, 2], [-1, 0], [-1, 1],
                     [2, 0], [0, 0]])
labels = np.array([1, 1, 0, 0, 1, 1, 0, 0, 1, 0])
classes = [0, 1]

bias = np.ones((features.shape[0], 1))
features = np.append(bias, features, axis = 1)
print(features.shape)

neural_network = NeuralNetwork(0.01)
print ('Random weights at the start of training')
print(neural_network.weight_matrix)
neural_network.train(features, np.expand_dims(labels, axis = 1), 30)

print ('New weights after training')
print (neural_network.weight_matrix)

"""f) The most optimal learning rate is 0.5, as it allows for a constant reduction of the cost function in a timely manner. Smaller learning rates lead to slower reduction of the cost function."""